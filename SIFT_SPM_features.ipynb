{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack = 'MD589'\n",
    "dm = DataManager(stack=stack)\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load sample locations\n",
    "\n",
    "patches_rootdir = '/home/yuncong/CSHL_data_patches/'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allROIs_allSections.h5'%{'stack':stack})\n",
    "indices_allROIs_allSections = pd.read_hdf(table_filepath, 'indices_allROIs_allSections')\n",
    "grid_parameters = pd.read_hdf(table_filepath, 'grid_parameters')\n",
    "\n",
    "patch_size, stride, w, h = grid_parameters.tolist()\n",
    "half_size = patch_size/2\n",
    "ys, xs = np.meshgrid(np.arange(half_size, h-half_size, stride), np.arange(half_size, w-half_size, stride),\n",
    "                 indexing='xy')\n",
    "sample_locations = np.c_[xs.flat, ys.flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, ymin, w, h = detect_bbox_lookup[stack]\n",
    "xmin = xmin * 32\n",
    "ymin = ymin * 32\n",
    "w = w * 32\n",
    "h = h * 32\n",
    "xmax = xmin + w - 1\n",
    "ymax = ymin + h - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "M = 200 # vocabulary size\n",
    "colors = np.vstack([(0,0,0), np.random.randint(0, 255, (M, 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary (as a sklearn.KMeans object)\n",
    "\n",
    "if os.path.exists(output_dir + '/vocab.pkl'):\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocabulary = joblib.load(output_dir + '/vocab.pkl')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    if os.path.exists(output_dir + '/sift_descriptors_pool_arr.bp'):\n",
    "        \n",
    "        # Load descriptor pool\n",
    "        descriptors_pool_arr = bp.unpack_ndarray_file(output_dir + '/sift_descriptors_pool_arr.bp')\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        vocabulary = KMeans(init='random', n_clusters=M, n_init=10)\n",
    "        vocabulary.fit(descriptors_pool_arr)\n",
    "\n",
    "        sys.stderr.write('sift: %.2f seconds\\n' % (time.time() - t)) # 300 seconds\n",
    "\n",
    "        cluster_centers = vocabulary.cluster_centers_\n",
    "\n",
    "        joblib.dump(vocabulary, output_dir + '/vocab.pkl')\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Generate SIFT descriptor pool\n",
    "        descriptors_pool = []\n",
    "\n",
    "        sift = cv2.SIFT();\n",
    "        \n",
    "        for sec in range(first_detect_sec, last_detect_sec+1, 10):\n",
    "\n",
    "            print sec\n",
    "\n",
    "            dm.set_slice(sec)\n",
    "            dm._load_image(versions=['rgb-jpg'])\n",
    "            img = dm.image_rgb_jpg[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "            keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "            n = 1000\n",
    "            random_indices = np.random.choice(range(len(descriptors)), n, replace=False)\n",
    "\n",
    "            descriptors_pool.append(descriptors[random_indices])\n",
    "\n",
    "        descriptors_pool_arr = np.vstack(descriptors_pool)\n",
    "        print len(descriptors_pool_arr), 'in descriptor pool'\n",
    "\n",
    "        bp.pack_ndarray_file(descriptors_pool_arr, output_dir + '/sift_descriptors_pool_arr.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift/MD589'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_if_not_exists(output_dir + '/%(stack)s' % {'stack': stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/PIL/Image.py:2224: DecompressionBombWarning: Image size (157463552 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "sift: 60.52 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288944 keypoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 11.09 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19064, 16, 200)\n",
      "(19064, 4, 200)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done in 14.741619 seconds\n",
      "sift: 61.42 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(19064, 200)\n",
      "296688 keypoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 11.34 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19328, 16, 200)\n",
      "(19328, 4, 200)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done in 15.345054 seconds\n",
      "sift: 60.04 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(19328, 200)\n",
      "282595 keypoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10.79 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19487, 16, 200)\n",
      "(19487, 4, 200)\n",
      "(19487, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done in 15.294611 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cb5a9e518fb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# 128 dim descriptor ï½ž 120 seconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sift: %.2f seconds\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sift = cv2.SIFT();\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "progress_bar = FloatProgress(min=first_detect_sec, max=last_detect_sec)\n",
    "display(progress_bar)\n",
    "\n",
    "for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "# for sec in [first_detect_sec]:\n",
    "    \n",
    "    progress_bar.value = sec\n",
    "    \n",
    "    labelmap_fp = output_dir + '/%(stack)s/%(stack)s_%(sec)04d_labelmap.hdf' % {'stack': stack, 'sec': sec}\n",
    "    \n",
    "    if os.path.exists(labelmap_fp):\n",
    "        \n",
    "        # Load labelmap\n",
    "        labelmap = load_hdf(labelmap_fp)\n",
    "    else:\n",
    "        \n",
    "        # Compute keypoints and assign labels\n",
    "\n",
    "        dm.set_slice(sec)\n",
    "        dm._load_image(versions=['rgb-jpg'])\n",
    "\n",
    "        img = dm.image_rgb_jpg[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "        t = time.time()\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None); # 128 dim descriptor ï½ž 120 seconds\n",
    "        sys.stderr.write('sift: %.2f seconds\\n' % (time.time() - t)) \n",
    "\n",
    "        keypoints_arr = np.array([k.pt for k in keypoints])\n",
    "        print len(keypoints), 'keypoints' # ~ 500k\n",
    "\n",
    "        t = time.time()\n",
    "        keypoint_labels = vocabulary.predict(descriptors)\n",
    "        sys.stderr.write('predict: %.2f seconds\\n' % (time.time() - t))  # ~ 20 s\n",
    "\n",
    "        # visualize keypoints (color indicating label)\n",
    "\n",
    "        # viz = img.copy()\n",
    "        # for (x, y), l in zip(keypoints_arr, keypoint_labels):\n",
    "        #     cv2.circle(viz, (int(x), int(y)), 3, colors[l], -1)\n",
    "        # display_image(viz)\n",
    "\n",
    "        # generate labelmap\n",
    "\n",
    "        labelmap = np.zeros(dm.image_rgb_jpg.shape[:2], np.int)\n",
    "        keypoints_arr_int = np.floor(keypoints_arr + (xmin, ymin)).astype(np.int)  # coords on original image\n",
    "        labelmap[keypoints_arr_int[:,1], keypoints_arr_int[:,0]] = keypoint_labels + 1\n",
    "\n",
    "        save_hdf(labelmap, labelmap_fp)\n",
    "        \n",
    "    \n",
    "    indices_roi = indices_allROIs_allSections[sec]['roi1']\n",
    "\n",
    "\n",
    "    # Compute histograms (method 2), for all levels\n",
    "\n",
    "    sample_locs_roi = sample_locations[indices_roi]\n",
    "\n",
    "    # compute level-2 histograms\n",
    "    l = 2\n",
    "\n",
    "    grid_size = patch_size / 2**l\n",
    "\n",
    "    if l == 2:\n",
    "        rx = [-2, -1, 0, 1]\n",
    "        ry = [-2, -1, 0, 1]\n",
    "    elif l == 1:\n",
    "        rx = [-1, 0]\n",
    "        ry = [-1, 0]\n",
    "    elif l == 0:\n",
    "        rx = [-.5]\n",
    "        ry = [-.5]\n",
    "\n",
    "    rxs, rys = np.meshgrid(rx, ry)\n",
    "\n",
    "    patch_coords_allGrid = []\n",
    "\n",
    "    for grid_i, (rx, ry) in enumerate(np.c_[rxs.flat, rys.flat]):\n",
    "\n",
    "        patch_xmin = sample_locs_roi[:,0] + rx * grid_size\n",
    "        patch_ymin = sample_locs_roi[:,1] + ry * grid_size\n",
    "        patch_xmax = sample_locs_roi[:,0] + (rx + 1) * grid_size\n",
    "        patch_ymax = sample_locs_roi[:,1] + (ry + 1) * grid_size\n",
    "\n",
    "        patch_coords_allGrid.append([patch_xmin, patch_ymin, patch_xmax, patch_ymax])\n",
    "\n",
    "\n",
    "    all_coords = np.hstack(patch_coords_allGrid)\n",
    "    patch_xmin = all_coords[0]\n",
    "    patch_ymin = all_coords[1]\n",
    "    patch_xmax = all_coords[2]\n",
    "    patch_ymax = all_coords[3]\n",
    "\n",
    "    def compute_histogram_particular_label(i):\n",
    "        m = (labelmap == i).astype(np.uint8)\n",
    "        mi = cv2.integral(m)\n",
    "        ci = mi[patch_ymin, patch_xmin] + mi[patch_ymax, patch_xmax] - mi[patch_ymax, patch_xmin] - mi[patch_ymin, patch_xmax]\n",
    "        return ci\n",
    "\n",
    "    t = time.time()\n",
    "    hists = Parallel(n_jobs=16)(delayed(compute_histogram_particular_label)(i) for i in range(1, M+1))\n",
    "    sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # ~ 18 seconds\n",
    "\n",
    "    n_grid = (2**l)**2\n",
    "    hists_arr2 = np.transpose(np.reshape(hists, (M, n_grid, -1)))\n",
    "    print hists_arr2.shape\n",
    "\n",
    "    # compute level-1 histograms based on level-2 histograms\n",
    "\n",
    "    hists_arr1 = np.transpose([hists_arr2[:, [0,1,4,5], :].sum(axis=1),\n",
    "                               hists_arr2[:, [2,3,6,7], :].sum(axis=1),\n",
    "                               hists_arr2[:, [8,9,12,13], :].sum(axis=1),\n",
    "                               hists_arr2[:, [10,11,14,15], :].sum(axis=1)], \n",
    "                              [1,0,2])\n",
    "    print hists_arr1.shape\n",
    "\n",
    "    # compute level-0 histograms based on level-1 histograms\n",
    "\n",
    "    hists_arr0 = hists_arr1.sum(axis=1)\n",
    "    print hists_arr0.shape\n",
    "    \n",
    "    bp.pack_ndarray_file(hists_arr0, output_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "    bp.pack_ndarray_file(hists_arr1, output_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "    bp.pack_ndarray_file(hists_arr2, output_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "    # save_hdf(hists_arr0, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l0.hdf' % {'stack': stack, 'sec': sec})\n",
    "    # save_hdf(hists_arr1, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l1.hdf' % {'stack': stack, 'sec': sec})\n",
    "    # save_hdf(hists_arr2, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l2.hdf' % {'stack': stack, 'sec': sec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelmap_viz = colors[labelmap]\n",
    "# display_image(labelmap_viz[5000:5500, 5000:5500])\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "plt.imshow(labelmap_viz[5000:5500, 5000:5500]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute histograms (method 1, slow)\n",
    "\n",
    "def f(x,y):\n",
    "    return np.bincount(labelmap[y-half_size:y+half_size-1, x-half_size:x+half_size-1].flat, minlength=M+1)[1:]\n",
    "\n",
    "t = time.time()\n",
    "sample_hists_list = []\n",
    "\n",
    "for s in range(0, len(indices_roi), 100):\n",
    "    res = Parallel(n_jobs=16)(delayed(f)(x,y) for x, y in sample_locations[indices_roi][s:s+100])\n",
    "    sample_hists_list.append(res)\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # ~ 72 seconds\n",
    "\n",
    "from itertools import chain\n",
    "sample_hists = list(chain(*sample_hists_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute histograms (method 2, fast), level-0\n",
    "\n",
    "sample_locs_roi = sample_locations[indices_roi]\n",
    "patch_xmin = sample_locs_roi[:,0] - half_size\n",
    "patch_ymin = sample_locs_roi[:,1] - half_size\n",
    "patch_xmax = sample_locs_roi[:,0] + half_size\n",
    "patch_ymax = sample_locs_roi[:,1] + half_size\n",
    "\n",
    "def compute_histogram_particular_label(i):\n",
    "    '''\n",
    "    Compute the histogram of label i using integral image.\n",
    "    '''\n",
    "    m = (labelmap == i).astype(np.uint8)\n",
    "    mi = cv2.integral(m)\n",
    "    ci = mi[patch_ymin, patch_xmin] + mi[patch_ymax, patch_xmax] - mi[patch_ymax, patch_xmin] - mi[patch_ymin, patch_xmax]\n",
    "    return ci\n",
    "\n",
    "t = time.time()\n",
    "hists = Parallel(n_jobs=16)(delayed(compute_histogram_particular_label)(i) for i in range(1, M+1))\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # ~ 18 seconds\n",
    "\n",
    "hists_arr = np.array(hists).T\n",
    "print hists_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute histograms (method 2), for all levels\n",
    "\n",
    "sample_locs_roi = sample_locations[indices_roi]\n",
    "\n",
    "# compute level-2 histograms\n",
    "l = 2\n",
    "\n",
    "grid_size = patch_size / 2**l\n",
    "\n",
    "if l == 2:\n",
    "    rx = [-2, -1, 0, 1]\n",
    "    ry = [-2, -1, 0, 1]\n",
    "elif l == 1:\n",
    "    rx = [-1, 0]\n",
    "    ry = [-1, 0]\n",
    "elif l == 0:\n",
    "    rx = [-.5]\n",
    "    ry = [-.5]\n",
    "\n",
    "rxs, rys = np.meshgrid(rx, ry)\n",
    "\n",
    "patch_coords_allGrid = []\n",
    "\n",
    "for grid_i, (rx, ry) in enumerate(np.c_[rxs.flat, rys.flat]):\n",
    "    \n",
    "    patch_xmin = sample_locs_roi[:,0] + rx * grid_size\n",
    "    patch_ymin = sample_locs_roi[:,1] + ry * grid_size\n",
    "    patch_xmax = sample_locs_roi[:,0] + (rx + 1) * grid_size\n",
    "    patch_ymax = sample_locs_roi[:,1] + (ry + 1) * grid_size\n",
    "    \n",
    "    patch_coords_allGrid.append([patch_xmin, patch_ymin, patch_xmax, patch_ymax])\n",
    "    \n",
    "    \n",
    "all_coords = np.hstack(patch_coords_allGrid)\n",
    "patch_xmin = all_coords[0]\n",
    "patch_ymin = all_coords[1]\n",
    "patch_xmax = all_coords[2]\n",
    "patch_ymax = all_coords[3]\n",
    "\n",
    "def compute_histogram_particular_label(i):\n",
    "    m = (labelmap == i).astype(np.uint8)\n",
    "    mi = cv2.integral(m)\n",
    "    ci = mi[patch_ymin, patch_xmin] + mi[patch_ymax, patch_xmax] - mi[patch_ymax, patch_xmin] - mi[patch_ymin, patch_xmax]\n",
    "    return ci\n",
    "\n",
    "t = time.time()\n",
    "hists = Parallel(n_jobs=16)(delayed(compute_histogram_particular_label)(i) for i in range(1, M+1))\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # ~ 18 seconds\n",
    "\n",
    "n_grid = (2**l)**2\n",
    "hists_arr2 = np.transpose(np.reshape(hists, (M, n_grid, -1)))\n",
    "print hists_arr2.shape\n",
    "\n",
    "# compute level-1 histograms based on level-2 histograms\n",
    "\n",
    "hists_arr1 = np.transpose([hists_arr2[:, [0,1,4,5], :].sum(axis=1),\n",
    "                           hists_arr2[:, [2,3,6,7], :].sum(axis=1),\n",
    "                           hists_arr2[:, [8,9,12,13], :].sum(axis=1),\n",
    "                           hists_arr2[:, [10,11,14,15], :].sum(axis=1)], \n",
    "                          [1,0,2])\n",
    "print hists_arr1.shape\n",
    "\n",
    "# compute level-0 histograms based on level-1 histograms\n",
    "\n",
    "hists_arr0 = hists_arr1.sum(axis=1)\n",
    "print hists_arr0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
