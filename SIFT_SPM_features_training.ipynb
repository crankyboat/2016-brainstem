{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack = 'MD589'\n",
    "dm = DataManager(stack=stack)\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load sample locations\n",
    "\n",
    "patches_rootdir = '/home/yuncong/CSHL_data_patches/'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allLandmarks_allSections.h5'%{'stack':stack})\n",
    "indices_allLandmarks_allSections = pd.read_hdf(table_filepath, 'indices_allLandmarks_allSections')\n",
    "grid_parameters = pd.read_hdf(table_filepath, 'grid_parameters')\n",
    "\n",
    "patch_size, stride, w, h = grid_parameters.tolist()\n",
    "half_size = patch_size/2\n",
    "ys, xs = np.meshgrid(np.arange(half_size, h-half_size, stride), np.arange(half_size, w-half_size, stride),\n",
    "                 indexing='xy')\n",
    "sample_locations = np.c_[xs.flat, ys.flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "n_labels = len(labels)\n",
    "\n",
    "labels_index = dict((j, i) for i, j in enumerate(labels))\n",
    "\n",
    "labels_from_surround = dict( (l+'_surround', l) for l in labels[1:])\n",
    "\n",
    "labels_surroundIncluded_list = labels[1:] + [l+'_surround' for l in labels[1:]]\n",
    "labels_surroundIncluded = set(labels_surroundIncluded_list)\n",
    "\n",
    "labels_surroundIncluded_index = dict((j, i) for i, j in enumerate(labels_surroundIncluded_list))\n",
    "\n",
    "# colors = np.random.randint(0, 255, (len(labels_index), 3))\n",
    "colors = np.loadtxt(os.environ['REPO_DIR'] + '/visualization/100colors.txt')\n",
    "colors[labels_index['BackG']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, ymin, w, h = detect_bbox_lookup[stack]\n",
    "xmin = xmin * 32\n",
    "ymin = ymin * 32\n",
    "w = w * 32\n",
    "h = h * 32\n",
    "xmax = xmin + w - 1\n",
    "ymax = ymin + h - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "M = 200 # vocabulary size\n",
    "colors = np.vstack([(0,0,0), np.random.randint(0, 255, (M, 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary (as a sklearn.KMeans object)\n",
    "\n",
    "if os.path.exists(output_dir + '/vocab.pkl'):\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocabulary = joblib.load(output_dir + '/vocab.pkl')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    if os.path.exists(output_dir + '/sift_descriptors_pool_arr.bp'):\n",
    "        \n",
    "        # Load descriptor pool\n",
    "        descriptors_pool_arr = bp.unpack_ndarray_file(output_dir + '/sift_descriptors_pool_arr.bp')\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        vocabulary = KMeans(init='random', n_clusters=M, n_init=10)\n",
    "        vocabulary.fit(descriptors_pool_arr)\n",
    "\n",
    "        sys.stderr.write('sift: %.2f seconds\\n' % (time.time() - t)) # 300 seconds\n",
    "\n",
    "        cluster_centers = vocabulary.cluster_centers_\n",
    "\n",
    "        joblib.dump(vocabulary, output_dir + '/vocab.pkl')\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Generate SIFT descriptor pool\n",
    "        descriptors_pool = []\n",
    "\n",
    "        sift = cv2.SIFT();\n",
    "        \n",
    "        for sec in range(first_detect_sec, last_detect_sec+1, 10):\n",
    "\n",
    "            print sec\n",
    "\n",
    "            dm.set_slice(sec)\n",
    "            dm._load_image(versions=['rgb-jpg'])\n",
    "            img = dm.image_rgb_jpg[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "            keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "            n = 1000\n",
    "            random_indices = np.random.choice(range(len(descriptors)), n, replace=False)\n",
    "\n",
    "            descriptors_pool.append(descriptors[random_indices])\n",
    "\n",
    "        descriptors_pool_arr = np.vstack(descriptors_pool)\n",
    "        print len(descriptors_pool_arr), 'in descriptor pool'\n",
    "\n",
    "        bp.pack_ndarray_file(descriptors_pool_arr, output_dir + '/sift_descriptors_pool_arr.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sift: 59.90 seconds\n",
      "predict: 11.08 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288944 keypoints\n"
     ]
    }
   ],
   "source": [
    "sift = cv2.SIFT();\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "progress_bar = FloatProgress(min=first_detect_sec, max=last_detect_sec)\n",
    "display(progress_bar)\n",
    "\n",
    "hists0_allLandmarks = defaultdict(list)\n",
    "hists1_allLandmarks = defaultdict(list)\n",
    "hists2_allLandmarks = defaultdict(list)\n",
    "\n",
    "for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "# for sec in [first_detect_sec]:\n",
    "    \n",
    "    progress_bar.value = sec\n",
    "    \n",
    "    labelmap_fp = output_dir + '/%(stack)s/%(stack)s_%(sec)04d_labelmap.hdf' % {'stack': stack, 'sec': sec}\n",
    "    \n",
    "    if os.path.exists(labelmap_fp):\n",
    "        \n",
    "        # Load labelmap\n",
    "        labelmap = load_hdf(labelmap_fp)\n",
    "    else:\n",
    "        \n",
    "        # Compute keypoints and assign labels\n",
    "\n",
    "        dm.set_slice(sec)\n",
    "        dm._load_image(versions=['rgb-jpg'])\n",
    "\n",
    "        img = dm.image_rgb_jpg[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "        t = time.time()\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None); # 128 dim descriptor ï½ž 120 seconds\n",
    "        sys.stderr.write('sift: %.2f seconds\\n' % (time.time() - t)) \n",
    "\n",
    "        keypoints_arr = np.array([k.pt for k in keypoints])\n",
    "        print len(keypoints), 'keypoints' # ~ 500k\n",
    "\n",
    "        t = time.time()\n",
    "        keypoint_labels = vocabulary.predict(descriptors)\n",
    "        sys.stderr.write('predict: %.2f seconds\\n' % (time.time() - t))  # ~ 20 s\n",
    "\n",
    "        # visualize keypoints (color indicating label)\n",
    "\n",
    "        # viz = img.copy()\n",
    "        # for (x, y), l in zip(keypoints_arr, keypoint_labels):\n",
    "        #     cv2.circle(viz, (int(x), int(y)), 3, colors[l], -1)\n",
    "        # display_image(viz)\n",
    "\n",
    "        # generate labelmap\n",
    "\n",
    "        labelmap = np.zeros(dm.image_rgb_jpg.shape[:2], np.int)\n",
    "        keypoints_arr_int = np.floor(keypoints_arr + (xmin, ymin)).astype(np.int)  # coords on original image\n",
    "        labelmap[keypoints_arr_int[:,1], keypoints_arr_int[:,0]] = keypoint_labels + 1\n",
    "\n",
    "        save_hdf(labelmap, labelmap_fp)\n",
    "        \n",
    "\n",
    "    # Compute histograms (method 2), for all levels\n",
    "\n",
    "    num_sample = 30\n",
    "    \n",
    "    for name in labels[1:]:\n",
    "        if name not in indices_allLandmarks_allSections[sec].dropna().index: \n",
    "            continue\n",
    "\n",
    "        indices = indices_allLandmarks_allSections[sec][name]\n",
    "        indices_sampled = np.random.choice(indices, min(len(indices), num_sample), replace=False)\n",
    "    \n",
    "        sample_locs_roi = sample_locations[indices_sampled]\n",
    "\n",
    "        # compute level-2 histograms\n",
    "        l = 2\n",
    "\n",
    "        grid_size = patch_size / 2**l\n",
    "\n",
    "        if l == 2:\n",
    "            rx = [-2, -1, 0, 1]\n",
    "            ry = [-2, -1, 0, 1]\n",
    "        elif l == 1:\n",
    "            rx = [-1, 0]\n",
    "            ry = [-1, 0]\n",
    "        elif l == 0:\n",
    "            rx = [-.5]\n",
    "            ry = [-.5]\n",
    "\n",
    "        rxs, rys = np.meshgrid(rx, ry)\n",
    "\n",
    "        patch_coords_allGrid = []\n",
    "\n",
    "        for grid_i, (rx, ry) in enumerate(np.c_[rxs.flat, rys.flat]):\n",
    "\n",
    "            patch_xmin = sample_locs_roi[:,0] + rx * grid_size\n",
    "            patch_ymin = sample_locs_roi[:,1] + ry * grid_size\n",
    "            patch_xmax = sample_locs_roi[:,0] + (rx + 1) * grid_size\n",
    "            patch_ymax = sample_locs_roi[:,1] + (ry + 1) * grid_size\n",
    "\n",
    "            patch_coords_allGrid.append([patch_xmin, patch_ymin, patch_xmax, patch_ymax])\n",
    "\n",
    "\n",
    "        all_coords = np.hstack(patch_coords_allGrid)\n",
    "        patch_xmin = all_coords[0]\n",
    "        patch_ymin = all_coords[1]\n",
    "        patch_xmax = all_coords[2]\n",
    "        patch_ymax = all_coords[3]\n",
    "\n",
    "        def compute_histogram_particular_label(i):\n",
    "            m = (labelmap == i).astype(np.uint8)\n",
    "            mi = cv2.integral(m)\n",
    "            ci = mi[patch_ymin, patch_xmin] + mi[patch_ymax, patch_xmax] - mi[patch_ymax, patch_xmin] - mi[patch_ymin, patch_xmax]\n",
    "            return ci\n",
    "\n",
    "        t = time.time()\n",
    "        hists = Parallel(n_jobs=16)(delayed(compute_histogram_particular_label)(i) for i in range(1, M+1))\n",
    "        sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # ~ 18 seconds\n",
    "\n",
    "        n_grid = (2**l)**2\n",
    "        hists_arr2 = np.transpose(np.reshape(hists, (M, n_grid, -1)))\n",
    "        print hists_arr2.shape\n",
    "\n",
    "        # compute level-1 histograms based on level-2 histograms\n",
    "\n",
    "        hists_arr1 = np.transpose([hists_arr2[:, [0,1,4,5], :].sum(axis=1),\n",
    "                                   hists_arr2[:, [2,3,6,7], :].sum(axis=1),\n",
    "                                   hists_arr2[:, [8,9,12,13], :].sum(axis=1),\n",
    "                                   hists_arr2[:, [10,11,14,15], :].sum(axis=1)], \n",
    "                                  [1,0,2])\n",
    "        print hists_arr1.shape\n",
    "\n",
    "        # compute level-0 histograms based on level-1 histograms\n",
    "\n",
    "        hists_arr0 = hists_arr1.sum(axis=1)\n",
    "        print hists_arr0.shape\n",
    "\n",
    "        hists2_allLandmarks[name].append(hists_arr2)\n",
    "        hists1_allLandmarks[name].append(hists_arr1)\n",
    "        hists0_allLandmarks[name].append(hists_arr0)\n",
    "    \n",
    "#     bp.pack_ndarray_file(hists_arr0, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "#     bp.pack_ndarray_file(hists_arr1, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "#     bp.pack_ndarray_file(hists_arr2, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "    # save_hdf(hists_arr0, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l0.hdf' % {'stack': stack, 'sec': sec})\n",
    "    # save_hdf(hists_arr1, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l1.hdf' % {'stack': stack, 'sec': sec})\n",
    "    # save_hdf(hists_arr2, output_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l2.hdf' % {'stack': stack, 'sec': sec})\n",
    "    \n",
    "hists0_allLandmarks.default_factory = None\n",
    "hists1_allLandmarks.default_factory = None\n",
    "hists2_allLandmarks.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hists0_allLandmarks = {name: np.concatenate(arrs) for name, arrs in hists0_allLandmarks.iteritems()}\n",
    "hists1_allLandmarks = {name: np.concatenate(arrs) for name, arrs in hists1_allLandmarks.iteritems()}\n",
    "hists2_allLandmarks = {name: np.concatenate(arrs) for name, arrs in hists2_allLandmarks.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, arr in hists0_allLandmarks.iteritems():\n",
    "    bp.pack_ndarray_file(arr, output_dir + '/train/%(stack)s_%(name)s_histograms_l0.bp' % {'stack': stack, 'name': name})\n",
    "\n",
    "for name, arr in hists1_allLandmarks.iteritems():\n",
    "    bp.pack_ndarray_file(arr, output_dir + '/train/%(stack)s_%(name)s_histograms_l1.bp' % {'stack': stack, 'name': name})\n",
    "\n",
    "for name, arr in hists2_allLandmarks.iteritems():\n",
    "    bp.pack_ndarray_file(arr, output_dir + '/train/%(stack)s_%(name)s_histograms_l2.bp' % {'stack': stack, 'name': name})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
