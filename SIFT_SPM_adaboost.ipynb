{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "n_labels = len(labels)\n",
    "\n",
    "labels_index = dict((j, i) for i, j in enumerate(labels))\n",
    "\n",
    "labels_from_surround = dict( (l+'_surround', l) for l in labels[1:])\n",
    "\n",
    "labels_surroundIncluded_list = labels[1:] + [l+'_surround' for l in labels[1:]]\n",
    "labels_surroundIncluded = set(labels_surroundIncluded_list)\n",
    "\n",
    "labels_surroundIncluded_index = dict((j, i) for i, j in enumerate(labels_surroundIncluded_list))\n",
    "\n",
    "# colors = np.random.randint(0, 255, (len(labels_index), 3))\n",
    "colors = np.loadtxt(os.environ['REPO_DIR'] + '/visualization/100colors.txt')\n",
    "colors[labels_index['BackG']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5N', 1440) 1000\n",
      "('7n', 3444) 1000\n",
      "('7N', 2579) 1000\n",
      "('12N', 1230) 1000\n",
      "('Pn', 3042) 1000\n",
      "('VLL', 1287) 1000\n",
      "('6N', 154) 154\n",
      "('Amb', 346) 346\n",
      "('R', 1082) 1000\n",
      "('Tz', 1387) 1000\n",
      "('RtTg', 2639) 1000\n",
      "('LRt', 1050) 1000\n",
      "('LC', 481) 481\n",
      "('AP', 483) 483\n",
      "('sp5', 3240) 1000\n"
     ]
    }
   ],
   "source": [
    "# build training data\n",
    "\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift'\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "n_sample = 1000\n",
    "\n",
    "for name in labels[1:]:\n",
    "    train_hists0 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l0.bp' % {'name': name})\n",
    "    train_hists1 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l1.bp' % {'name': name})\n",
    "    train_hists2 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l2.bp' % {'name': name})\n",
    "    \n",
    "    n_train = train_hists0.shape[0]\n",
    "    print (name, n_train),\n",
    "\n",
    "    #     train_hists = np.c_[train_hists0, train_hists1.reshape((n_train, -1)), train_hists2.reshape((n_train, -1))]\n",
    "\n",
    "    random_indices = np.random.choice(range(n_train), min(n_train, n_sample), replace=False)\n",
    "    n_train = len(random_indices)\n",
    "    train_hists = np.c_[train_hists0[random_indices], \n",
    "                        train_hists1[random_indices].reshape((n_train, -1)), \n",
    "                        train_hists2[random_indices].reshape((n_train, -1))]\n",
    "    \n",
    "    train_data.append(train_hists)\n",
    "    train_labels.append(np.ones((n_train, )) * labels_index[name])\n",
    "    print n_train\n",
    "    \n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "n_train = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12464, 4200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_data_normalized = train_data / train_data.sum(axis=1)[:,None].astype(np.float)\n",
    "train_data_normalized = np.nan_to_num(train_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp.pack_ndarray_file(train_data_normalized, 'preprocessed/train_data.bp')\n",
    "# bp.pack_ndarray_file(train_data, 'preprocessed/train_data.bp')\n",
    "bp.pack_ndarray_file(train_labels, 'preprocessed/train_labels.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build testing data\n",
    "\n",
    "stack = 'MD585'\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "sec = first_detect_sec\n",
    "\n",
    "test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "n_test = test_hists0.shape[0]\n",
    "test_hists = np.c_[test_hists0, test_hists1.reshape((n_test, -1)), test_hists2.reshape((n_test, -1))]\n",
    "\n",
    "test_data = test_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20979, 4200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_normalized = test_data / test_data.sum(axis=1)[:,None].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp.pack_ndarray_file(test_data_normalized, 'preprocessed/%(stack)s_test_data.bp' % {'stack': stack})\n",
    "# bp.pack_ndarray_file(test_data, 'preprocessed/%(stack)s_test_data.bp' % {'stack': stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 192.439459 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, j1):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized[i], train_data_normalized[j1:]).sum(axis=1)\n",
    "#     dist = np.minimum(train_data[i], train_data[j1:]).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "train_dist_triangle = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, i) \n",
    "                                     for i in range(n_train)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 200s / 12k training data\n",
    "\n",
    "train_dist_mat = np.empty((n_train, n_train))\n",
    "train_dist_mat[np.triu_indices(n_train)] = train_dist_triangle\n",
    "r = np.tril_indices(n_train)\n",
    "train_dist_mat[r] = train_dist_mat.T[r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000318 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(h):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized, h).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "# train_dist_mat = np.array(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(h) \n",
    "#                                      for h in train_data_normalized[:1000]))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000340 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 3\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, ni, j, nj):\n",
    "    dists = np.minimum(train_data_normalized[i:i+ni, None], train_data_normalized[j:j+nj]).sum(axis=-1)\n",
    "    return dists\n",
    "\n",
    "# train_dist_mat = np.empty((n_train, n_train))\n",
    "# ni = 100\n",
    "# nj = 100\n",
    "# for j in range(0, n_train, nj):\n",
    "#     train_dist_mat[:, j:j+nj] = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, ni, j, nj) \n",
    "#                                                     for i in range(0, n_train, ni)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12464, 12464)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dist_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp.pack_ndarray_file(train_dist_mat, 'preprocessed/train_spm_dist_mat.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hypersphere_abstain:\n",
    "    \n",
    "    def __init__(self, center, radius, threshold, label, alpha, acc, unweighted_acc,\n",
    "                 label_distribution, center_index, neighbor_indices):\n",
    "        \n",
    "        self.center = center\n",
    "        self.radius = radius\n",
    "        self.thres = threshold\n",
    "        self.label = label\n",
    "        self.alpha = alpha\n",
    "        self.acc = acc # Weighted accuracy\n",
    "        self.unweighted_acc = unweighted_acc\n",
    "        self.label_distribution = label_distribution\n",
    "        self.center_index = center_index\n",
    "        self.neighbor_indices = neighbor_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t:  0\n",
      "exemplar label:  1.0\n",
      "interest label:  1\n",
      "percentage:  0.28\n",
      "Abstain\n",
      "\n",
      "t:  1\n",
      "exemplar label:  6.0\n",
      "interest label:  6\n",
      "percentage:  0.3\n",
      "Abstain\n",
      "\n",
      "t:  2\n",
      "exemplar label:  4.0\n",
      "interest label:  4\n",
      "percentage:  0.36\n",
      "Abstain\n",
      "\n",
      "t:  3\n",
      "exemplar label:  10.0\n",
      "interest label:  10\n",
      "percentage:  0.32\n",
      "Abstain\n",
      "\n",
      "t:  4\n",
      "exemplar label:  12.0\n",
      "interest label:  4\n",
      "percentage:  0.3\n",
      "Abstain\n",
      "\n",
      "t:  5\n",
      "exemplar label:  6.0\n",
      "interest label:  4\n",
      "percentage:  0.4\n",
      "Abstain\n",
      "\n",
      "t:  6\n",
      "exemplar label:  12.0\n",
      "interest label:  5\n",
      "percentage:  0.24\n",
      "Abstain\n",
      "\n",
      "t:  7\n",
      "exemplar label:  12.0\n",
      "interest label:  6\n",
      "percentage:  0.22\n",
      "Abstain\n",
      "\n",
      "t:  8\n",
      "exemplar label:  2.0\n",
      "interest label:  2\n",
      "percentage:  0.6\n",
      "(60, 0.61666666666666725)\n",
      "weighted_acc:  0.616666666667\n",
      "alpha:  0.237711848308\n",
      "acc:  0.616666666667\n",
      "[ 0  0 37  0  0  0  0  0  0  0  0  1  0  0  0 22]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEJ5JREFUeJzt3X2sZHV9x/H3B9ciD5YiIKtsdMXQaNVFDSpKq6OGQsDa\nNqHWttCm0drU2tIHRUNr9kLbtNCIREz7R1VSQEQxRkFhpa1eLD5E6gILrlgttoiG5UFsISGUut/+\nMYft7e69OzN3Zpi5v32/ksmeOXPOnM/ch8/97W9mzqSqkCStffvNOoAkaTIsdElqhIUuSY2w0CWp\nERa6JDXCQpekRgxd6En2S7I1yVXd9Y1JvpRkW5KPJFk3vZiSpEFGGaGfCWxfcv19wHlVtQnYAbxt\nksEkSaMZqtCTbABOAT7QXX8C8PKq+lS3yWXA66aSUJI0lGFH6O8F3gE89rbSpwL3Lrn9LuCoCeaS\nJI1oYKEnORXYUVU3A1l609RSSZJGNswTmScAr09yCnAA8GTgfOCwJdtsoD9K30MSTxYjSatQVSMN\nnAeO0Kvq7Kp6RlUdDbwR+FxVnQF8JcnPd5udDly7l/uY+8vmzZtnnsGcZjSnOR+7rMY4r0M/E3hX\nkm3AeuCiMe5LkjSmkV47XlXXA9d3y98BXj6NUJKk0flO0U6v15t1hKGYc3LWQkYw56StlZyrkdXO\n1Qx9gKSmfQxJak0SatJPikqS1gYLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQI\nC12SGmGhd9av30iSiV7Wr98464claR/iuVw6Sfi/T9ib2L2u+rzGkvZtnstFkvZhFrokNcJCl6RG\nWOiS1AgLXZIaMbDQk+yf5MYkW5N8M8kF3fqLk9yR5Kbutk3TjytJWsnAD4muqkeSvLKqHk7yBOCL\nSXrdzW+vqk9MNaEkaShDTblU1cPd4v7dPvd010d6jaQkaXqGKvQk+yW5CbgbWKyq7d1Nf55ke5KL\nkvzY1FJKkgYadoS+s6peBGwAXpnkVcBZVfVc4FjgQODd04spSRpk4Bz6UlX1X0k+AxxfVdd36x5N\n8gFg80r7LSws7Fru9Xr0er1VhZWkVi0uLrK4uDjWfQw8l0uSw4BHquqhJAcAnwXOA75aVfemfxKU\n84EnVNUfLbO/53KRpBGt5lwuw4zQnw5c0i88ngRcXlWfSfK5JIcCBwA3A28ZNbAkaXI822LHEbqk\neeLZFiVpH2ahS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5J\njbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqxMBCT7J/khuTbE3yzSQXdOs3JvlSkm1J\nPpJkmA+cliRNycBCr6pHgFdW1YuBnwJekeTVwPuA86pqE7ADeNtUk0qS9mqoKZeqerhb3L/bZwdw\nfFV9qlt/GfC6yceTJA1rqEJPsl+Sm4C7gUXgAeC+JZvcBRw18XSSpKENNe9dVTuBFyX5ceCzwM2j\nHGRhYWHXcq/Xo9frjbK7JDVvcXGRxcXFse4jVTXaDsm7gQJ+v6qe2q07DvjLqjpxme1r1GPMQhL6\nD2ui98paeOyS5k8Sqiqj7DPMq1wOS3Jwt3wAcCJwE/CVJL/QbXY6cO2IeSVJEzRwhJ7kBcAl3dUn\nAZdX1Z8leRZwOXAQsB04o6oeXWZ/R+iSNKLVjNBHnnIZlYU+/49d0vyZypSLJGltsNAlqREWuiQ1\nwkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMs\ndElqhIUuSY2w0CWpERa6JDViYKEn2ZDk+iS3Jrk9yTu69ZuT3JVka3c5efpxJUkrGfgh0UmOBI6o\nqtuSHAxsBU4DfhF4sKouGLC/HxItSSNazYdErxu0QVXtAHZ0yw8l2QYc9dgxR04pSZqKkebQk2wE\njgNu6Fa9Ncn2JJcmOXTC2SRJIxg4Qn9MN91yJXBmVT2Y5P3AuVVVSc4BLgJOX27fhYWFXcu9Xo9e\nrzdOZklqzuLiIouLi2Pdx8A5dIAk64BPA1uq6sJlbn8a8Pmqes4ytzmHLkkjWs0c+rBTLh8Cti8t\n8yRHLLn9NGD7KAeWJE3WMK9yOQH4AnAr/SFsAWcDvwZsAp4I3Am8qaq+t8z+jtAlaUSrGaEPNeUy\nDgt9/h+7pPkzzSkXSdKcs9AlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakR\nFrokNcJCl6RGWOiSNKb16zeSZKKX1fBsix3PtihptabYH55tUZL2RRa6JDXCQpekRljoktQIC12S\nGjGw0JNsSHJ9kluT3J7krG79oUmuS3JLki1JDpl+XEnSSga+bDHJkcARVXVbkoOBrwG/BLwZuKOq\nLkzyB8CzqurMZfb3ZYuSmrZmXrZYVTuq6rZu+SHgVmADcCpwabfZZd11SdKMjDSHnmQjcBzwz/RH\n7fcDVNV9wBGTDidJGt66YTfspluuBM6sqgeTDP3/i4WFhV3LvV6PXq83QkRJ2hcsdpfVG+qt/0nW\nAZ8GtlTVhd26bwMvq6r7kxwOfLmqjllmX+fQJTVtzcyhdz4EbH+szDvXAGd0y2cA145yYEnSZA3z\nKpcTgC/QfzK0usvZwFeBjwJHAncDb6iqHy6zvyN0SU2blxG6Z1vsWOiSVmteCt13ikpSIyx0SWqE\nhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljo\nktQIC12SGmGhS1IjLHRJaoSFLkmNGFjoST6YZEeSbUvWbU5yV5Kt3eXk6caUJA0yzAj9YuCkZdZf\nUFUv7i5bJpxLkjSigYVeVTcADyxz00ifRi1Jmq5x5tDfmmR7kkuTHDqxRJKkVVm3yv3eD5xbVZXk\nHOAi4PSVNl5YWNi13Ov16PV6qzysJLVqsbusXqpq8EbJM4Grq2rTMrc9Dfh8VT1nhX1rmGPMWhJg\n0jnDWnjsksYzxf4YaWp72CmXsGTOPMkRS247Ddg+ykElSZM3cMolyeVADzgsyZ3AZuA1STYBTwTu\nBN40zZCSpMGGmnIZ6wBOuUz4PiXNm7U25SJJmnMWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqE\nhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRgws\n9CQfTLIjybYl6w5Ncl2SW5JsSXLIdGNKkgYZZoR+MXDSbuvOAa6pqmOBLcC5kw4mSRrNwEKvqhuA\nB3ZbfSpwabd8WXddkjRDq51DP7yq7geoqvuAIyYXSZK0Gusej4MsLCzsWu71evR6vcfjsJK0hix2\nl9VLVQ3eKHkmcHVVbequfxt4WVXdn+Rw4MtVdcwK+9Ywx5i1JMCkc4a18NgljWeK/ZFR9hh2yiXd\n5THXAGd0y2cA145yUEnS5A0coSe5HOgBhwE7gM3AJ4GPAUcCdwNvqKofrrC/I3RJTZuXEfpQUy7j\nsNDn/7FLGs+8FLrvFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLU\nCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1Ih14+yc5N+B/wR2Ao9W1UsnEUqS\nNLqxCp1+kfeq6oFJhJEkrd64Uy6ZwH1IkiZg3DLeCVyX5JYkb5tEIEnS6ow75fLyqronyRHAliTf\nqKp/mkQwSdJoxir0qrqn+/feJB8HXgLsUegLCwu7lnu9Hr1eb5zDSlKDFrvL6qWqVrdjciBQVfVw\nkoOAa4D3VNVVu21Xqz3G4ykJMOmcYS08dknjmWJ/ZJQ9xhmhHwl8MslO4EDgit3LXJL0+Fn1CH3o\nAzhCn/B9Spo38zJC9yWHktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtd\nkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFL+6D16zeSZKKX9es3zvph\n7fPG+kzRJCcDf03/D8MlVXXeMtv4maLSnPHnfbLW/GeKJvkx4G+Bk4BjgdOSvHC196fhLC4uzjrC\nUNZCzrWQEdZOzrWi5a/nOFMuLwNuq6rvV9X/AB8FTp1MLK1krfwwroWcayEjrJ2ca0XLX89xCn0D\n8N0l1+/q1kmSZsAnRSWpEat+UjTJzwDvrKrXddffDuxfVX+x23b75rMkkjSmUZ8UXTfGsb4KPC/J\n04F7gV8GfnvcQJKk1Vl1oVfVI0l+B7gOCHBpVW2dWDJJ0kjGeh26JGl+TO1J0SQnJ7k1ydeTvHNa\nxxlHkg1Jru9y3p7krFln2psk+yXZmuSqWWdZSZJDknwsyS1Jtic5ftaZlpPknCT/muQbSa5McsCs\nMwEk+WCSHUm2LVl3aJLruq/pliSHzDJjl2m5nO/pvudfT3J1kqfMMmOXaY+cS2774yQ75zlnkt/r\nvu/bkpw/6H6mUuhr6E1HjwK/W1UvAI4D3pxk04wz7c2ZwPZZhxjg74BPVNWxwPOBr884zx6SPBs4\nA3h+VT0X2An8ymxT7XIx/d+bpc4Brum+pluAcx/3VHtaLufV9L+mz6P/ff/Txz3VnpbLSZINwInA\nfzzuiZa3R84kpwA/C7y4qjYBfzXoTqY1Ql8Tbzqqqh1VdVu3/BCwDThqtqmW1/0AngJ8YNZZVtKN\ndF5YVVcAVNXOqnpwxrGW8wPgv4GDkqwDDgTunG2kvqq6AXhgt9WnApd2y5cxB79Ly+WsqsWq2tld\nvYE5+F1a4esJ8F7gHY9znBWtkPO3gPOr6kfdNj8YdD/TKvQ196ajJBvpj9JvmG2SFT32AzjPT3oc\nA9zXTbncluTvkxw061C7q6oHgPfQL/HvAT+sqn+cbaq9Oryq7geoqvuAI2acZxhvAeZyajDJ64Hv\nVtWts84ywHOAk5LcnORLSV4xaAffWAQkORi4EjhzHkeUSU4FdlTVzfRfUTSvLwXdD3gJ/VHF8+mP\nON4920h7SnI08IfAM4GnAwcn+dXZpmpHkj8BHq2qD886y+6650rOBjYvXT2jOIPsBzy5ql5If7r1\nivTPArbXHabhLuAZS65v6NbNne6/3B8HPlxVn5p1nhWcALw+yR3AR4BXJ7lkxpmW813grqr6l+76\nx4F5fO7kpcAXq+oH3X9nPwH89Iwz7c29SQ4DSHI4cM+M86woyW/QnxKa1z+QzwY2Arck+Q79bvpa\nkqfONNXy7qT/s0lV3Uh/mvDIve0wrULf9aajJE+k/6aja6d0rHF9CNheVRfOOshKqursqnpGVR0N\nvBH4XFX9+qxz7a6q7qI/5XJMt+q1wO0zjLSSfwOOT3JAN+J5bbduXuz+v7Br6D+JS/fvvPwu/b+c\n3em0zwJ+rqoemVmqPe3KWVW3VdX6qjq6qp5Ff6D5oqqahz+Su3/fPwO8BiDJTwIHMOiPeVVN5QKc\nDNxG/9nud03rOGNmPAH4EXAzcBOwFTh51rkGZH4VcNWsc+wl37HAjd33/hrg0FlnWiHnZuBb9P/g\nXAE8adaZulyXA98HHqE/QvtN4FDgH+g/aX8d8BNzmvNb9F81srW7/M085tzt9juAp8xjTvpv/Ly0\n+126FThx0P34xiJJaoRPikpSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIa8b+Yw6qH\neTVDYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x627efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t:  9\n",
      "exemplar label:  1.0\n",
      "interest label:  4\n",
      "percentage:  0.48\n",
      "Abstain\n",
      "(1, array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Adaboost.M1\n",
    "\n",
    "# Load preprocessed data\n",
    "X = bp.unpack_ndarray_file('preprocessed/train_data.bp')\n",
    "y = bp.unpack_ndarray_file('preprocessed/train_labels.bp')\n",
    "train_kernel = bp.unpack_ndarray_file('preprocessed/train_spm_dist_mat.bp')\n",
    "\n",
    "# Initialization\n",
    "n_samples = X.shape[0]\n",
    "D = 1.0 / n_samples * np.ones((n_samples, ))\n",
    "T = 10\n",
    "H = []\n",
    "\n",
    "# WeakLearner initialization\n",
    "init_radius = 50\n",
    "thres_abstain = 0.5\n",
    "growth_radius = 10\n",
    "\n",
    "for t in xrange(0, T):\n",
    "    \n",
    "    print '\\nt: ', t\n",
    "\n",
    "    # Choose an exemplar\n",
    "    exemplar_index = np.random.choice(range(n_samples), p=D.tolist())\n",
    "    exemplar = X[exemplar_index]\n",
    "#     print 'exemplar: ', exemplar_index\n",
    "    print 'exemplar label: ', y[exemplar_index]\n",
    "    \n",
    "    # Calculate dist to all other\n",
    "    scores = train_kernel[exemplar_index]\n",
    "    sorted_scores = np.sort(scores)[::-1]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_X = X[sorted_indices]\n",
    "    sorted_y = y[sorted_indices].astype('int64')\n",
    "    sorted_D = D[sorted_indices]\n",
    "#     print sorted_y[:10]\n",
    "#     print sorted_scores[:10]\n",
    "#     print sorted_scores[-10:]\n",
    "    \n",
    "    # Calculate plurarity of initial sphere & abstain\n",
    "    count = np.bincount(sorted_y[:init_radius],\n",
    "                        weights=sorted_D[:init_radius],\n",
    "                        minlength=16)\n",
    "    unweighted_count = np.bincount(sorted_y[:init_radius],\n",
    "                                   minlength=16)\n",
    "    y_of_interest = np.argmax(count)\n",
    "#     print count, np.sum(sorted_D[:init_radius]), len(count)\n",
    "#     print unweighted_count, len(count)\n",
    "    print 'interest label: ', y_of_interest\n",
    "    \n",
    "    percentage = count[y_of_interest]*1.0 / np.sum(sorted_D[:init_radius])\n",
    "    print 'percentage: ', percentage\n",
    "    \n",
    "    # Abstain test\n",
    "    if percentage<thres_abstain:\n",
    "        print \"Abstain\"\n",
    "        continue\n",
    "    \n",
    "    # Caculate best radius\n",
    "    index_of_interest = np.array([y_==y_of_interest for y_ in sorted_y])\n",
    "    weight_of_interest = np.cumsum(index_of_interest*sorted_D)\n",
    "    best_radius = init_radius\n",
    "    best_percentage = percentage\n",
    "    for i in range(init_radius+growth_radius, n_samples, growth_radius):\n",
    "        perc = weight_of_interest[i-1]*1.0 / np.sum(sorted_D[:i])\n",
    "        if perc>best_percentage:\n",
    "            best_radius = i\n",
    "            best_percentage = perc\n",
    "#         print (i, perc)\n",
    "    print (best_radius, best_percentage)\n",
    "    \n",
    "    # Alpha\n",
    "    w_total = np.sum(sorted_D[:best_radius])\n",
    "    w_acc = weight_of_interest[best_radius-1]*1.0 / w_total\n",
    "    w_err = 1.0 - w_acc\n",
    "#     print 'weighted_total: ', w_total\n",
    "    print 'weighted_acc: ', w_acc\n",
    "#     print 'weighted_err: ', w_err\n",
    "    \n",
    "    epsilon = 1.0e-10\n",
    "    alpha = 0.5 * math.log((w_acc+epsilon)/(w_err+epsilon))\n",
    "    print 'alpha: ', alpha\n",
    "    \n",
    "    # Reweight\n",
    "    index_of_other = ~index_of_interest\n",
    "#     print 'labels: ', y[:20]\n",
    "    D[sorted_indices] = D[sorted_indices] + index_of_interest*math.exp(-1.0*alpha)\n",
    "#     print 'interest: ', D[:20]\n",
    "    D[sorted_indices] = D[sorted_indices] + index_of_other*math.exp(alpha)\n",
    "#     print 'others: ', D[:20]\n",
    "    \n",
    "    # Normalize\n",
    "    D = D*1.0 / np.sum(D)\n",
    "#     print 'reweighted sum: ', np.sum(D)\n",
    "    \n",
    "    # Append WeakLearner to list\n",
    "    acc = np.sum(index_of_interest[:best_radius])*1.0 / best_radius\n",
    "    threshold = sorted_scores[best_radius-1]\n",
    "    label_distribution = np.bincount(sorted_y[:best_radius], minlength=16)\n",
    "    print 'acc: ', acc\n",
    "    print label_distribution\n",
    "    plt.bar(np.arange(0,16), label_distribution, width=1)\n",
    "    plt.show()\n",
    "    \n",
    "    h = Hypersphere_abstain(exemplar, best_radius, threshold, y_of_interest,\n",
    "                            alpha, w_acc, acc, label_distribution,\n",
    "                            exemplar_index, sorted_indices)\n",
    "    H.append(h)\n",
    "    \n",
    "\n",
    "# Predict\n",
    "\n",
    "result = []\n",
    "n_samples = train_kernel.shape[1] # n_test\n",
    "\n",
    "for i in range(1, len(H)+1):\n",
    "    \n",
    "    classifiers = H[:i]\n",
    "    prob = np.zeros((n_samples, 16))\n",
    "    for clf in classifiers:   \n",
    "        score = np.copy(train_kernel[clf.center_index])\n",
    "        score[score < clf.thres] = 0\n",
    "        score[score >= clf.thres] = clf.alpha\n",
    "        prob[:,clf.label] += score\n",
    "    max_prob = np.max(prob, axis=1)\n",
    "    labels = np.argmax(prob, axis=1)\n",
    "    labels[max_prob==0] = 0\n",
    "    print (i, y[:10], max_prob[:10], labels[:10])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adaboost.M2\n",
    "\n",
    "# train_data = train_data\n",
    "# train_labels = train_labels\n",
    "\n",
    "# n_train = train_data.shape[0]\n",
    "\n",
    "# D = 1.0 / n_train * np.ones((n_train, ))\n",
    "\n",
    "# W = np.ones((n_train, ))\n",
    "\n",
    "# choose exemplar\n",
    "\n",
    "# compute accuracy of exemplar\n",
    "\n",
    "# choose the plurality\n",
    "\n",
    "# compute threshold\n",
    "\n",
    "# reweight samples\n",
    "\n",
    "# store hypersphere\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
