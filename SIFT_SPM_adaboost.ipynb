{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "n_labels = len(labels)\n",
    "\n",
    "labels_index = dict((j, i) for i, j in enumerate(labels))\n",
    "\n",
    "labels_from_surround = dict( (l+'_surround', l) for l in labels[1:])\n",
    "\n",
    "labels_surroundIncluded_list = labels[1:] + [l+'_surround' for l in labels[1:]]\n",
    "labels_surroundIncluded = set(labels_surroundIncluded_list)\n",
    "\n",
    "labels_surroundIncluded_index = dict((j, i) for i, j in enumerate(labels_surroundIncluded_list))\n",
    "\n",
    "# colors = np.random.randint(0, 255, (len(labels_index), 3))\n",
    "colors = np.loadtxt(os.environ['REPO_DIR'] + '/visualization/100colors.txt')\n",
    "colors[labels_index['BackG']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12464, 4200)\n"
     ]
    }
   ],
   "source": [
    "# build training data\n",
    "\n",
    "sift_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "n_sample = 1000\n",
    "\n",
    "for name in labels[1:]:\n",
    "    train_hists0 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l0.bp' % {'name': name})\n",
    "    train_hists1 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l1.bp' % {'name': name})\n",
    "    train_hists2 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l2.bp' % {'name': name})\n",
    "    \n",
    "    n_train = train_hists0.shape[0]\n",
    "\n",
    "    #     train_hists = np.c_[train_hists0, train_hists1.reshape((n_train, -1)), train_hists2.reshape((n_train, -1))]\n",
    "\n",
    "    random_indices = np.random.choice(range(n_train), min(n_train, n_sample), replace=False)\n",
    "    n_train = len(random_indices)\n",
    "    train_hists = np.c_[train_hists0[random_indices], \n",
    "                        train_hists1[random_indices].reshape((n_train, -1)), \n",
    "                        train_hists2[random_indices].reshape((n_train, -1))]\n",
    "    \n",
    "    train_data.append(train_hists)\n",
    "    train_labels.append(np.ones((n_train, )) * labels_index[name])\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "n_train = train_data.shape[0]\n",
    "    \n",
    "print train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_data_normalized = train_data / train_data.sum(axis=1)[:,None].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build testing data\n",
    "\n",
    "stack = 'MD585'\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "sec = first_detect_sec\n",
    "\n",
    "test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "n_test = test_hists0.shape[0]\n",
    "test_hists = np.c_[test_hists0, test_hists1.reshape((n_test, -1)), test_hists2.reshape((n_test, -1))]\n",
    "\n",
    "test_data = test_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20979, 4200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_normalized = test_data / test_data.sum(axis=1)[:,None].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 192.517379 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, j1):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized[i], train_data_normalized[j1:]).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "train_dist_triangle = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, i) \n",
    "                                     for i in range(n_train)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 200s / 12k training data\n",
    "\n",
    "train_dist_mat = np.empty((n_train, n_train))\n",
    "train_dist_mat[np.triu_indices(n_train)] = train_dist_triangle\n",
    "r = np.tril_indices(n_train)\n",
    "train_dist_mat[r] = train_dist_mat.T[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method 2\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(h):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized, h).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "# train_dist_mat = np.array(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(h) \n",
    "#                                      for h in train_data_normalized[:1000]))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 369.700769 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 3\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, ni, j, nj):\n",
    "    dists = np.minimum(train_data_normalized[i:i+ni, None], train_data_normalized[j:j+nj]).sum(axis=-1)\n",
    "    return dists\n",
    "\n",
    "train_dist_mat = np.empty((n_train, n_train))\n",
    "ni = 100\n",
    "nj = 100\n",
    "for j in range(0, n_train, nj):\n",
    "    train_dist_mat[:, j:j+nj] = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, ni, j, nj) \n",
    "                                                    for i in range(0, n_train, ni)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp.pack_ndarray_file(train_dist_mat, sift_dir + '/train/train_spm_dist_mat.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hypersphere_abstain:\n",
    "    \n",
    "    def __init__(self, histogram, threshold, label, alpha, size, acc, index, \n",
    "                 unweighted_acc, label_distribution, neighbor_indices):\n",
    "        \n",
    "        self.h = histogram\n",
    "        self.t = threshold\n",
    "        self.l = label\n",
    "        self.alpha = alpha\n",
    "        self.size = size\n",
    "        self.acc = acc # Weighted accuracy\n",
    "        self.index = index\n",
    "        self.unweighted_acc = unweighted_acc\n",
    "        self.label_distribution = label_distribution\n",
    "        self.neighbor_indices = neighbor_indices        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adaboost.M2\n",
    "\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "D = 1./ n_train * np.ones((n_train, ))\n",
    "\n",
    "W = np.ones((n_train, ))\n",
    "\n",
    "# choose exemplar\n",
    "\n",
    "# compute accuracy of exemplar\n",
    "\n",
    "# choose the plurality\n",
    "\n",
    "# compute threshold\n",
    "\n",
    "# reweight samples\n",
    "\n",
    "# store hypersphere\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
