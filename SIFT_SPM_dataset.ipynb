{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "n_labels = len(labels)\n",
    "\n",
    "labels_index = dict((j, i) for i, j in enumerate(labels))\n",
    "\n",
    "labels_from_surround = dict( (l+'_surround', l) for l in labels[1:])\n",
    "\n",
    "labels_surroundIncluded_list = labels[1:] + [l+'_surround' for l in labels[1:]]\n",
    "labels_surroundIncluded = set(labels_surroundIncluded_list)\n",
    "\n",
    "labels_surroundIncluded_index = dict((j, i) for i, j in enumerate(labels_surroundIncluded_list))\n",
    "\n",
    "# colors = np.random.randint(0, 255, (len(labels_index), 3))\n",
    "colors = np.loadtxt(os.environ['REPO_DIR'] + '/visualization/100colors.txt')\n",
    "colors[labels_index['BackG']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5N', 1440) Train:  1152 Test:  288\n",
      "('7n', 3444) Train:  1200 Test:  300\n",
      "('7N', 2579) Train:  1200 Test:  300\n",
      "('12N', 1230) Train:  984 Test:  246\n",
      "('Pn', 3042) Train:  1200 Test:  300\n",
      "('VLL', 1287) Train:  1029 Test:  258\n",
      "('6N', 154) Train:  123 Test:  31\n",
      "('Amb', 346) Train:  276 Test:  70\n",
      "('R', 1082) Train:  865 Test:  217\n",
      "('Tz', 1387) Train:  1109 Test:  278\n",
      "('RtTg', 2639) Train:  1200 Test:  300\n",
      "('LRt', 1050) Train:  840 Test:  210\n",
      "('LC', 481) Train:  384 Test:  97\n",
      "('AP', 483) Train:  386 Test:  97\n",
      "('sp5', 3240) Train:  1200 Test:  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# build training data\n",
    "\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift'\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "train_fnames = []\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "test_fnames = []\n",
    "\n",
    "n_sample = 1500\n",
    "\n",
    "for name in labels[1:]:\n",
    "\n",
    "    hists0 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l0.bp' % {'name': name})\n",
    "    hists1 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l1.bp' % {'name': name})\n",
    "    hists2 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l2.bp' % {'name': name})\n",
    "    files = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_fnames.bp' % {'name': name})\n",
    "#     print (hists0.shape, hists1.shape, hists2.shape, files.shape)\n",
    "    \n",
    "    n_total = hists0.shape[0]\n",
    "    print (name, n_total),\n",
    "\n",
    "    random_indices = np.random.choice(range(n_total), min(n_total, n_sample), replace=False)\n",
    "    \n",
    "    frac = int(0.8*len(random_indices))\n",
    "    random_train = random_indices[:frac]\n",
    "    random_test = random_indices[frac:]\n",
    "    n_train = len(random_train)\n",
    "    n_test = len(random_test)\n",
    "    \n",
    "#     train_hists = np.c_[train_hists0[random_indices], \n",
    "#                         train_hists1[random_indices].reshape((n_train, -1)), \n",
    "#                         train_hists2[random_indices].reshape((n_train, -1))]\n",
    "    train_hists = np.c_[hists0[random_train], \n",
    "                        hists1[random_train].reshape((n_train, -1)), \n",
    "                        hists2[random_train].reshape((n_train, -1))]\n",
    "    test_hists =  np.c_[hists0[random_test], \n",
    "                        hists1[random_test].reshape((n_test, -1)), \n",
    "                        hists2[random_test].reshape((n_test, -1))]\n",
    "    \n",
    "#     train_files = train_files_[random_indices].reshape((n_train,))\n",
    "    train_files = files[random_train].reshape((n_train,))\n",
    "    test_files = files[random_test].reshape((n_test,))\n",
    "    \n",
    "    train_data.append(train_hists)\n",
    "    train_labels.append(np.ones((n_train, )) * labels_index[name])\n",
    "    train_fnames.append(train_files)\n",
    "    \n",
    "    test_data.append(test_hists)\n",
    "    test_labels.append(np.ones((n_test, )) * labels_index[name])\n",
    "    test_fnames.append(test_files)\n",
    "    \n",
    "    print 'Train: ', n_train, \n",
    "    print 'Test: ', n_test\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "train_fnames = np.concatenate(train_fnames)\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "test_data = np.concatenate(test_data)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_fnames = np.concatenate(test_fnames)\n",
    "n_test = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13148, 4200)\n",
      "(13148,)\n",
      "(13148,) |S255\n",
      "/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg/train/MD589/MD589_5N_0300_0023.png\n",
      "(3292, 4200)\n",
      "(3292,)\n",
      "(3292,) |S255\n",
      "/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg/train/MD589/MD589_5N_0297_0014.png\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print train_labels.shape\n",
    "print train_fnames.shape, train_fnames.dtype\n",
    "print train_fnames[0]\n",
    "\n",
    "print test_data.shape\n",
    "print test_labels.shape\n",
    "print test_fnames.shape, test_fnames.dtype\n",
    "print test_fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n",
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:4: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "train_data_normalized = train_data / train_data.sum(axis=1)[:,None].astype(np.float)\n",
    "train_data_normalized = np.nan_to_num(train_data_normalized)\n",
    "\n",
    "test_data_normalized = test_data / test_data.sum(axis=1)[:,None].astype(np.float)\n",
    "test_data_normalized = np.nan_to_num(test_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pack time: 1.090234 seconds\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# bp.pack_ndarray_file(train_data_normalized, 'preprocessed/train_data.bp')\n",
    "# bp.pack_ndarray_file(train_labels, 'preprocessed/train_labels.bp')\n",
    "# bp.pack_ndarray_file(train)\n",
    "\n",
    "bp.pack_ndarray_file(train_data_normalized, sift_dir+'/preprocessed/train_data.bp')\n",
    "bp.pack_ndarray_file(train_labels, sift_dir+'/preprocessed/train_labels.bp')\n",
    "bp.pack_ndarray_file(train_fnames, sift_dir+'/preprocessed/train_fnames.bp', chunk_size=255*4000)\n",
    "\n",
    "bp.pack_ndarray_file(test_data_normalized, sift_dir+'/preprocessed/test_data.bp')\n",
    "bp.pack_ndarray_file(test_labels, sift_dir+'/preprocessed/test_labels.bp')\n",
    "bp.pack_ndarray_file(test_fnames, sift_dir+'/preprocessed/test_fnames.bp', chunk_size=255*4000)\n",
    "\n",
    "sys.stderr.write('Pack time: %f seconds\\n' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 213.189468 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, j1):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized[i], train_data_normalized[j1:]).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "train_dist_triangle = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, i) \n",
    "                                     for i in range(n_train)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 200s / 12k training data\n",
    "\n",
    "train_dist_mat = np.empty((n_train, n_train))\n",
    "train_dist_mat[np.triu_indices(n_train)] = train_dist_triangle\n",
    "r = np.tril_indices(n_train)\n",
    "train_dist_mat[r] = train_dist_mat.T[r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000201 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(h):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized, h).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "# train_dist_mat = np.array(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(h) \n",
    "#                                      for h in train_data_normalized[:1000]))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000246 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 3\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, ni, j, nj):\n",
    "    dists = np.minimum(train_data_normalized[i:i+ni, None], train_data_normalized[j:j+nj]).sum(axis=-1)\n",
    "    return dists\n",
    "\n",
    "# train_dist_mat = np.empty((n_train, n_train))\n",
    "# ni = 100\n",
    "# nj = 100\n",
    "# for j in range(0, n_train, nj):\n",
    "#     train_dist_mat[:, j:j+nj] = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, ni, j, nj) \n",
    "#                                                     for i in range(0, n_train, ni)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13148, 13148)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dist_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pack training kernel\n",
    "\n",
    "# bp.pack_ndarray_file(train_dist_mat, 'preprocessed/train_spm_dist_mat.bp')\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "bp.pack_ndarray_file(train_dist_mat, sift_dir+'/preprocessed/train_spm_dist_mat.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # build testing data\n",
    "\n",
    "# # MY ADDITION\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg/test'\n",
    "# # MY ADDITION END\n",
    "\n",
    "# stack = 'MD585'\n",
    "\n",
    "# first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "# sec = first_detect_sec\n",
    "\n",
    "# # test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "# # test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "# # test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "# test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "# test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "# test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "# # MY ADDITION\n",
    "# test_fcoords = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_fcoords.bp' % {'stack': stack, 'sec': sec})\n",
    "# # MY ADDITION END\n",
    "\n",
    "# n_test = test_hists0.shape[0]\n",
    "# test_hists = np.c_[test_hists0, test_hists1.reshape((n_test, -1)), test_hists2.reshape((n_test, -1))]\n",
    "\n",
    "# test_data = test_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print test_data.shape\n",
    "# print test_fcoords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_data_normalized = test_data / test_data.sum(axis=1)[:,None].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # bp.pack_ndarray_file(test_data_normalized, 'preprocessed/%(stack)s_test_data.bp' % {'stack': stack})\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "# bp.pack_ndarray_file(test_data_normalized, sift_dir+'/preprocessed/%(stack)s_test_data.bp' % {'stack': stack})\n",
    "# bp.pack_ndarray_file(test_fcoords, sift_dir+'/preprocessed/%(stack)s_test_fcoords.bp' % {'stack': stack})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
