{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "n_labels = len(labels)\n",
    "\n",
    "labels_index = dict((j, i) for i, j in enumerate(labels))\n",
    "\n",
    "labels_from_surround = dict( (l+'_surround', l) for l in labels[1:])\n",
    "\n",
    "labels_surroundIncluded_list = labels[1:] + [l+'_surround' for l in labels[1:]]\n",
    "labels_surroundIncluded = set(labels_surroundIncluded_list)\n",
    "\n",
    "labels_surroundIncluded_index = dict((j, i) for i, j in enumerate(labels_surroundIncluded_list))\n",
    "\n",
    "# colors = np.random.randint(0, 255, (len(labels_index), 3))\n",
    "colors = np.loadtxt(os.environ['REPO_DIR'] + '/visualization/100colors.txt')\n",
    "colors[labels_index['BackG']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5N', 1440) 100\n",
      "('7n', 3444) 100\n",
      "('7N', 2579) 100\n",
      "('12N', 1230) 100\n",
      "('Pn', 3042) 100\n",
      "('VLL', 1287) 100\n",
      "('6N', 154) 100\n",
      "('Amb', 346) 100\n",
      "('R', 1082) 100\n",
      "('Tz', 1387) 100\n",
      "('RtTg', 2639) 100\n",
      "('LRt', 1050) 100\n",
      "('LC', 481) 100\n",
      "('AP', 483) 100\n",
      "('sp5', 3240) 100\n"
     ]
    }
   ],
   "source": [
    "# build training data\n",
    "\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/yuncong/Brain/learning/sift'\n",
    "# sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift'\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "train_fnames = []\n",
    "\n",
    "# n_sample = 1000\n",
    "n_sample = 100\n",
    "\n",
    "for name in labels[1:]:\n",
    "# for name in ['7n', '5N', 'sp5', 'VLL']:\n",
    "# for name in ['7n', '5N', 'sp5']:\n",
    "\n",
    "    train_hists0 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l0.bp' % {'name': name})\n",
    "    train_hists1 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l1.bp' % {'name': name})\n",
    "    train_hists2 = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_histograms_l2.bp' % {'name': name})\n",
    "    \n",
    "    train_files = bp.unpack_ndarray_file(sift_dir + '/train/MD589_%(name)s_fnames.bp' % {'name': name})\n",
    "#     print (train_hists0.shape, train_hists1.shape, train_hists2.shape, train_files.shape)\n",
    "    \n",
    "    n_train = train_hists0.shape[0]\n",
    "    print (name, n_train),\n",
    "\n",
    "    #     train_hists = np.c_[train_hists0, train_hists1.reshape((n_train, -1)), train_hists2.reshape((n_train, -1))]\n",
    "\n",
    "    random_indices = np.random.choice(range(n_train), min(n_train, n_sample), replace=False)\n",
    "    n_train = len(random_indices)\n",
    "    train_hists = np.c_[train_hists0[random_indices], \n",
    "                        train_hists1[random_indices].reshape((n_train, -1)), \n",
    "                        train_hists2[random_indices].reshape((n_train, -1))]\n",
    "    train_files = train_files[random_indices].reshape((n_train,))\n",
    "    \n",
    "    train_data.append(train_hists)\n",
    "    train_labels.append(np.ones((n_train, )) * labels_index[name])\n",
    "    train_fnames.append(train_files)\n",
    "    print n_train\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "train_fnames = np.concatenate(train_fnames)\n",
    "n_train = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 4200)\n",
      "(1500,)\n",
      "(1500,) |S255\n",
      "/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg/train/MD589/MD589_5N_0299_0009.png\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print train_labels.shape\n",
    "print train_fnames.shape, train_fnames.dtype\n",
    "print train_fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_data_normalized = train_data / train_data.sum(axis=1)[:,None].astype(np.float)\n",
    "train_data_normalized = np.nan_to_num(train_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bp.pack_ndarray_file(train_data_normalized, 'preprocessed/train_data.bp')\n",
    "# bp.pack_ndarray_file(train_labels, 'preprocessed/train_labels.bp')\n",
    "# bp.pack_ndarray_file(train)\n",
    "bp.pack_ndarray_file(train_data_normalized, sift_dir+'/preprocessed/train_data.bp')\n",
    "bp.pack_ndarray_file(train_labels, sift_dir+'/preprocessed/train_labels.bp')\n",
    "bp.pack_ndarray_file(train_fnames, sift_dir+'/preprocessed/train_fnames.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 2.523323 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, j1):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized[i], train_data_normalized[j1:]).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "train_dist_triangle = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, i) \n",
    "                                     for i in range(n_train)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 200s / 12k training data\n",
    "\n",
    "train_dist_mat = np.empty((n_train, n_train))\n",
    "train_dist_mat[np.triu_indices(n_train)] = train_dist_triangle\n",
    "r = np.tril_indices(n_train)\n",
    "train_dist_mat[r] = train_dist_mat.T[r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000202 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(h):\n",
    "#     dist = .5 * np.sum(train_data_normalized + h - np.abs(train_data_normalized - h), axis=1)\n",
    "    dist = np.minimum(train_data_normalized, h).sum(axis=1)\n",
    "    return dist\n",
    "\n",
    "# train_dist_mat = np.array(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(h) \n",
    "#                                      for h in train_data_normalized[:1000]))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute intersection kernel: 0.000273 seconds\n"
     ]
    }
   ],
   "source": [
    "# method 3\n",
    "t = time.time()\n",
    "\n",
    "def compute_intersection_kernel_oneJob(i, ni, j, nj):\n",
    "    dists = np.minimum(train_data_normalized[i:i+ni, None], train_data_normalized[j:j+nj]).sum(axis=-1)\n",
    "    return dists\n",
    "\n",
    "# train_dist_mat = np.empty((n_train, n_train))\n",
    "# ni = 100\n",
    "# nj = 100\n",
    "# for j in range(0, n_train, nj):\n",
    "#     train_dist_mat[:, j:j+nj] = np.concatenate(Parallel(n_jobs=16)(delayed(compute_intersection_kernel_oneJob)(i, ni, j, nj) \n",
    "#                                                     for i in range(0, n_train, ni)))\n",
    "\n",
    "sys.stderr.write('compute intersection kernel: %f seconds\\n' % (time.time() - t)) # ~ 400s / 12k training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1500)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dist_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bp.pack_ndarray_file(train_dist_mat, 'preprocessed/train_spm_dist_mat.bp')\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "bp.pack_ndarray_file(train_dist_mat, sift_dir+'/preprocessed/train_spm_dist_mat.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build testing data\n",
    "\n",
    "# MY ADDITION\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg/test'\n",
    "# MY ADDITION END\n",
    "\n",
    "stack = 'MD585'\n",
    "\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "sec = first_detect_sec\n",
    "\n",
    "# test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "# test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "# test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists0 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l0.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists1 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l1.bp' % {'stack': stack, 'sec': sec})\n",
    "test_hists2 = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_histograms_l2.bp' % {'stack': stack, 'sec': sec})\n",
    "\n",
    "# MY ADDITION\n",
    "test_fcoords = bp.unpack_ndarray_file(sift_dir + '/%(stack)s_%(sec)04d_roi1_fcoords.bp' % {'stack': stack, 'sec': sec})\n",
    "# MY ADDITION END\n",
    "\n",
    "n_test = test_hists0.shape[0]\n",
    "test_hists = np.c_[test_hists0, test_hists1.reshape((n_test, -1)), test_hists2.reshape((n_test, -1))]\n",
    "\n",
    "test_data = test_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20979, 4200)\n",
      "(20979, 6)\n"
     ]
    }
   ],
   "source": [
    "print test_data.shape\n",
    "print test_fcoords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_normalized = test_data / test_data.sum(axis=1)[:,None].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bp.pack_ndarray_file(test_data_normalized, 'preprocessed/%(stack)s_test_data.bp' % {'stack': stack})\n",
    "sift_dir = '/oasis/projects/nsf/csd395/wel144/2016-brainstem/sift-jpeg'\n",
    "bp.pack_ndarray_file(test_data_normalized, sift_dir+'/preprocessed/%(stack)s_test_data.bp' % {'stack': stack})\n",
    "bp.pack_ndarray_file(test_fcoords, sift_dir+'/preprocessed/%(stack)s_test_fcoords.bp' % {'stack': stack})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
